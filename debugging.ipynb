{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ptan\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from lib.dqn_model import DQN\n",
    "from lib.common import unpack_batch, batch_generator\n",
    "from typing import List, Optional, Tuple, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"PongNoFrameskip-v4\")\n",
    "env.reset()\n",
    "random_step = env.action_space.sample()\n",
    "(obser, reward, is_done, _) = env.step(random_step)\n",
    "print(obser.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ptan.common.wrappers.LazyFrames'>\n",
      "(4, 84, 84)\n",
      "Box(4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"PongNoFrameskip-v4\")\n",
    "env = ptan.common.wrappers.wrap_dqn(env)\n",
    "env.reset()\n",
    "(obser, reward, is_done, _) = env.step(random_step)\n",
    "print(type(obser))\n",
    "obser = np.array(obser)\n",
    "print(obser.shape)\n",
    "print(env.observation_space)\n",
    "# We notice that wrapper changes the observation. More wrappers can be found in the module 'wrappers.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExperienceFirstLast(state=<ptan.common.wrappers.LazyFrames object at 0x7fdb89aa7a58>, action=2, reward=0.0, last_state=<ptan.common.wrappers.LazyFrames object at 0x7fdb89aa7518>) \n",
      "\n",
      "ExperienceFirstLast(state=<ptan.common.wrappers.LazyFrames object at 0x7fdb89aa7518>, action=5, reward=0.0, last_state=<ptan.common.wrappers.LazyFrames object at 0x7fdb88951278>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the neural network and the target network\n",
    "net = DQN(env.observation_space.shape,env.action_space.n)\n",
    "tgt_net = ptan.agent.TargetNet(net)\n",
    "\n",
    "# Create an agent based on the nn and the selector\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1.0)\n",
    "agent = ptan.agent.DQNAgent(net, selector)\n",
    "\n",
    "# The ExperienceSourceFirstLast is used to generate trajectories. It returns (state, action, reward, last_state) \n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=0.99)\n",
    "#exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=0.99, steps_count=4)\n",
    "\n",
    "# Create a buffer, the buffer_size is only 1 \n",
    "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=2)\n",
    "\n",
    "buffer.populate(2)\n",
    "batch = buffer.sample(2)\n",
    "for i in batch:\n",
    "    print(i,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 84, 84)\n",
      "torch.Size([2, 4, 84, 84])\n",
      "torch.Size([2, 4, 84, 84])\n",
      "tensor([[-0.0326,  0.0243,  0.0364,  0.0376, -0.0048, -0.0450],\n",
      "        [-0.0326,  0.0243,  0.0363,  0.0377, -0.0048, -0.0449]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([2, 5]) torch.Size([2])\n",
      "tensor([False, False])\n"
     ]
    }
   ],
   "source": [
    "states, actions, rewards, dones, next_states = unpack_batch(batch) \n",
    "print(states.shape) # The output 2 for batch and the other three are the observations\n",
    "\n",
    "states_v = torch.tensor(states)\n",
    "next_states_v = torch.tensor(next_states)\n",
    "actions_v = torch.tensor(actions)\n",
    "rewards_v = torch.tensor(rewards)\n",
    "done_mask = torch.BoolTensor(dones)\n",
    "\n",
    "print(states_v.shape)\n",
    "print(next_states_v.shape)\n",
    "# The input to our DQN should be (batch_size, 4, 84, 84)\n",
    "out = net(states_v)\n",
    "print(out)\n",
    "print(actions_v, actions_v.shape)\n",
    "print(done_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2],\n",
      "        [5]])\n",
      "torch.Size([2, 1])\n",
      "tensor([[ 0.0364],\n",
      "        [-0.0449]], grad_fn=<GatherBackward>)\n",
      "tensor([ 0.0364, -0.0449], grad_fn=<SqueezeBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' We are simply selecting the state-action values using squeeze, unsqueeze and gather options '"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_v_un = actions_v.unsqueeze(-1)\n",
    "print(actions_v_un)\n",
    "print(actions_v_un.shape)\n",
    "\n",
    "state_action_vals = out.gather(1, actions_v_un) # The 1 is for 1st dimension\n",
    "print(state_action_vals)\n",
    "print(state_action_vals.squeeze(-1))\n",
    "\n",
    "\"\"\" We are simply selecting the state-action values using squeeze, unsqueeze and gather options \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0326,  0.0243,  0.0363,  0.0377, -0.0048, -0.0449],\n",
      "        [-0.0325,  0.0243,  0.0364,  0.0376, -0.0049, -0.0449]])\n",
      "tensor([0.0377, 0.0376])\n",
      "tensor([0.0377, 0.0376])\n",
      "tensor([0.0377, 0.0376])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # Telling torch not to store any gradient transforms, since we are only obtaining data\n",
    "    print(tgt_net.target_model(next_states_v))\n",
    "    next_state_vals = tgt_net.target_model(next_states_v).max(1)[0] # max(1) to get max along dimension 1\n",
    "    print(next_state_vals)\n",
    "    next_state_vals[done_mask] = 0.0\n",
    "    print(next_state_vals)\n",
    "    print(next_state_vals.detach())\n",
    "    \n",
    "bellman_vals = next_state_vals.detach() * 0.99 + rewards_v # 0.99 -> gamma value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0034, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(nn.MSELoss()(state_action_vals, bellman_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExperienceFirstLast(state=<ptan.common.wrappers.LazyFrames object at 0x7fdb8a2314e0>, action=5, reward=0.0, last_state=<ptan.common.wrappers.LazyFrames object at 0x7fdb88951390>) \n",
      "\n",
      "ExperienceFirstLast(state=<ptan.common.wrappers.LazyFrames object at 0x7fdb8a2314a8>, action=5, reward=0.0, last_state=<ptan.common.wrappers.LazyFrames object at 0x7fdb889512b0>) \n",
      "\n",
      "ExperienceFirstLast(state=<ptan.common.wrappers.LazyFrames object at 0x7fdb89a8f780>, action=2, reward=0.0, last_state=<ptan.common.wrappers.LazyFrames object at 0x7fdb88951978>) \n",
      "\n",
      "ExperienceFirstLast(state=<ptan.common.wrappers.LazyFrames object at 0x7fdb89a8f898>, action=1, reward=0.0, last_state=<ptan.common.wrappers.LazyFrames object at 0x7fdb88951780>) \n",
      "\n",
      "ExperienceFirstLast(state=<ptan.common.wrappers.LazyFrames object at 0x7fdb88951390>, action=5, reward=0.0, last_state=<ptan.common.wrappers.LazyFrames object at 0x7fdb88951208>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=0.99, steps_count=4)\n",
    "\n",
    "# Create a buffer, the buffer_size is only 1 \n",
    "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=5)\n",
    "\n",
    "buffer.populate(5)\n",
    "batch = buffer.sample(5)\n",
    "for i in batch:\n",
    "    print(i,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 84, 84)\n",
      "torch.Size([2, 4, 84, 84])\n",
      "torch.Size([2, 4, 84, 84])\n",
      "tensor([[-0.0323,  0.0241,  0.0362,  0.0375, -0.0050, -0.0450],\n",
      "        [-0.0319,  0.0240,  0.0361,  0.0377, -0.0049, -0.0448]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([2, 1]) torch.Size([2])\n",
      "tensor([False, False])\n"
     ]
    }
   ],
   "source": [
    "states, actions, rewards, dones, next_states = unpack_batch(batch) \n",
    "print(states.shape) # The output 2 for batch and the other three are the observations\n",
    "\n",
    "states_v = torch.tensor(states)\n",
    "next_states_v = torch.tensor(next_states)\n",
    "actions_v = torch.tensor(actions)\n",
    "rewards_v = torch.tensor(rewards)\n",
    "done_mask = torch.BoolTensor(dones)\n",
    "\n",
    "print(states_v.shape)\n",
    "print(next_states_v.shape)\n",
    "out = net(states_v)\n",
    "print(out)\n",
    "print(actions_v, actions_v.shape)\n",
    "print(done_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation is that the batch generated from steps_count=4 is giving the similar output as that with single step.\n",
    "Shouldn't there be difference is values of "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
